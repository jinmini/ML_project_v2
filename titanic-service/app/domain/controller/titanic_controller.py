from app.domain.service.titanic_service import TitanicService
from app.domain.service.modeling_service import ModelingService
import pandas as pd
import os
from sklearn.ensemble import GradientBoostingClassifier

class Controller :
     
    service = TitanicService()
    modeling_service = ModelingService()
    '''
    print(f'ê²°ì •íŠ¸ë¦¬ í™œìš©í•œ ê²€ì¦ ì •í™•ë„ {None}')
    print(f'ëœë¤í¬ë ˆìŠ¤íŠ¸ í™œìš©í•œ ê²€ì¦ ì •í™•ë„ {None}')
    print(f'ë‚˜ì´ë¸Œë² ì´ì¦ˆ í™œìš©í•œ ê²€ì¦ ì •í™•ë„ {None}')
    print(f'KNN í™œìš©í•œ ê²€ì¦ ì •í™•ë„ {None}')
    print(f'SVM í™œìš©í•œ ê²€ì¦ ì •í™•ë„ {None}')
    '''

    def preprocess(self, train_fname, test_fname): #ë°ì´í„° ì „ì²˜ë¦¬
        return self.service.preprocess(train_fname, test_fname)
    
    def learning(self): #ëª¨ë¸ í•™ìŠµ
        """
        ì—¬ëŸ¬ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì„ í•™ìŠµí•˜ê³  ê° ëª¨ë¸ì˜ ì •í™•ë„ë¥¼ ê³„ì‚°
        
        Returns:
            dict: ëª¨ë¸ë³„ ì •í™•ë„ë¥¼ ë‹´ì€ ì‚¬ì „
        """
        print("\n" + "="*50)
        print("ğŸ“Š íƒ€ì´íƒ€ë‹‰ ìƒì¡´ì ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ ì‹œì‘ ğŸ“Š".center(50))
        print("="*50)
        this = self.service.dataset
        
        # ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸(ë¡œì§€ìŠ¤í‹± íšŒê·€) ì •í™•ë„ ê³„ì‚°
        print("\nğŸ” ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸(ë¡œì§€ìŠ¤í‹± íšŒê·€) í•™ìŠµ ì¤‘...")
        baseline_accuracy = self.modeling_service.accuracy_by_logistic_regression(this)
        print(f"âœ… ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸ ì •í™•ë„: {baseline_accuracy:.4f}")
        
        # ê° ì•Œê³ ë¦¬ì¦˜ë³„ ì •í™•ë„ ê³„ì‚°
        accuracy = {}
        accuracy['ë² ì´ìŠ¤ë¼ì¸(ë¡œì§€ìŠ¤í‹± íšŒê·€)'] = baseline_accuracy
        
        print("\nğŸ” ì˜ì‚¬ê²°ì • íŠ¸ë¦¬ ëª¨ë¸ í•™ìŠµ ì¤‘...")
        tree_acc = self.modeling_service.accuracy_by_dtree(this)
        accuracy['ê²°ì •íŠ¸ë¦¬'] = tree_acc
        if tree_acc > baseline_accuracy:
            print(f"âœ… ì˜ì‚¬ê²°ì • íŠ¸ë¦¬ ì •í™•ë„: {tree_acc:.4f} (ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„: {tree_acc - baseline_accuracy:+.4f})")
        else:
            print(f"âŒ ì˜ì‚¬ê²°ì • íŠ¸ë¦¬ ì •í™•ë„: {tree_acc:.4f} (ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„: {tree_acc - baseline_accuracy:+.4f})")
        
        print("\nğŸ” ëœë¤ í¬ë ˆìŠ¤íŠ¸ ëª¨ë¸ í•™ìŠµ ì¤‘...")
        rf_acc = self.modeling_service.accuracy_by_random_forest(this)
        accuracy['ëœë¤í¬ë ˆìŠ¤íŠ¸'] = rf_acc
        if rf_acc > baseline_accuracy:
            print(f"âœ… ëœë¤ í¬ë ˆìŠ¤íŠ¸ ì •í™•ë„: {rf_acc:.4f} (ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„: {rf_acc - baseline_accuracy:+.4f})")
        else:
            print(f"âŒ ëœë¤ í¬ë ˆìŠ¤íŠ¸ ì •í™•ë„: {rf_acc:.4f} (ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„: {rf_acc - baseline_accuracy:+.4f})")
        
        print("\nğŸ” ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ëª¨ë¸ í•™ìŠµ ì¤‘...")
        nb_acc = self.modeling_service.accuracy_by_naive_bayes(this)
        accuracy['ë‚˜ì´ë¸Œë² ì´ì¦ˆ'] = nb_acc
        if nb_acc > baseline_accuracy:
            print(f"âœ… ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ì •í™•ë„: {nb_acc:.4f} (ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„: {nb_acc - baseline_accuracy:+.4f})")
        else:
            print(f"âŒ ë‚˜ì´ë¸Œ ë² ì´ì¦ˆ ì •í™•ë„: {nb_acc:.4f} (ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„: {nb_acc - baseline_accuracy:+.4f})")
        
        print("\nğŸ” K-ìµœê·¼ì ‘ ì´ì›ƒ ëª¨ë¸ í•™ìŠµ ì¤‘...")
        knn_acc = self.modeling_service.accuracy_by_knn(this)
        accuracy['KNN'] = knn_acc
        if knn_acc > baseline_accuracy:
            print(f"âœ… KNN ì •í™•ë„: {knn_acc:.4f} (ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„: {knn_acc - baseline_accuracy:+.4f})")
        else:
            print(f"âŒ KNN ì •í™•ë„: {knn_acc:.4f} (ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„: {knn_acc - baseline_accuracy:+.4f})")
        
        print("\nğŸ” ì„œí¬íŠ¸ ë²¡í„° ë¨¸ì‹  ëª¨ë¸ í•™ìŠµ ì¤‘...")
        svm_acc = self.modeling_service.accuracy_by_svm(this)
        accuracy['SVM'] = svm_acc
        if svm_acc > baseline_accuracy:
            print(f"âœ… SVM ì •í™•ë„: {svm_acc:.4f} (ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„: {svm_acc - baseline_accuracy:+.4f})")
        else:
            print(f"âŒ SVM ì •í™•ë„: {svm_acc:.4f} (ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„: {svm_acc - baseline_accuracy:+.4f})")

        print("\nğŸ” ê·¸ë ˆë””ì–¸íŠ¸ ë¶€ìŠ¤íŒ… ëª¨ë¸ í•™ìŠµ ì¤‘...")
        gb_acc = self.modeling_service.accuracy_by_gradient_boosting(this)
        accuracy['GradientBoosting'] = gb_acc
        if gb_acc > baseline_accuracy:
            print(f"âœ… GradientBoosting ì •í™•ë„: {gb_acc:.4f} (ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„: {gb_acc - baseline_accuracy:+.4f})")
        else:
            print(f"âŒ GradientBoosting ì •í™•ë„: {gb_acc:.4f} (ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„: {gb_acc - baseline_accuracy:+.4f})")
        
        # ê²°ê³¼ ì €ì¥
        this.accuracy = accuracy
        
        return accuracy

    def evaluation(self): #ëª¨ë¸ í‰ê°€
        """
        í•™ìŠµëœ ëª¨ë¸ë“¤ì˜ ì •í™•ë„ë¥¼ ë¹„êµí•˜ì—¬ ìµœì ì˜ ëª¨ë¸ ì„ íƒ
        
        Returns:
            tuple: ê°€ì¥ ë†’ì€ ì •í™•ë„ì™€ í•´ë‹¹ ëª¨ë¸ ì´ë¦„
        """
        print("\n" + "="*50)
        print("ğŸ“Š íƒ€ì´íƒ€ë‹‰ ìƒì¡´ì ì˜ˆì¸¡ ëª¨ë¸ í‰ê°€ ê²°ê³¼ ğŸ“Š".center(50))
        print("="*50)
        this = self.service.dataset
        
        if not hasattr(this, 'accuracy'):
            print("âŒ ëª¨ë¸ì´ í•™ìŠµë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € learning() ë©”ì†Œë“œë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            return None, None
        
        # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì°¾ê¸°
        best_model = max(this.accuracy.items(), key=lambda x: x[1])
        
        print(f"\nğŸ† ìµœê³  ì„±ëŠ¥ ëª¨ë¸: {best_model[0]}, ì •í™•ë„: {best_model[1]:.4f}")
        print("\nğŸ“‹ ëª¨ë¸ë³„ ì •í™•ë„ ë¹„êµ:")
        
        # ë² ì´ìŠ¤ë¼ì¸ê³¼ ë¹„êµí•œ ì„±ëŠ¥ í–¥ìƒ ì¶œë ¥
        baseline_accuracy = this.accuracy.get('ë² ì´ìŠ¤ë¼ì¸(ë¡œì§€ìŠ¤í‹± íšŒê·€)', 0)
        
        # ì •ë ¬ëœ ê²°ê³¼ í‘œì‹œ
        sorted_results = sorted(this.accuracy.items(), key=lambda x: x[1], reverse=True)
        
        print("\n" + "-"*60)
        print(f"{'ëª¨ë¸ëª…':<20} {'ì •í™•ë„':<10} {'ë² ì´ìŠ¤ë¼ì¸ ëŒ€ë¹„':<15} {'í‰ê°€'}")
        print("-"*60)
        
        for i, (model, acc) in enumerate(sorted_results):
            if model == 'ë² ì´ìŠ¤ë¼ì¸(ë¡œì§€ìŠ¤í‹± íšŒê·€)':
                if i == 0:  # ë² ì´ìŠ¤ë¼ì¸ì´ ê°€ì¥ ì¢‹ì€ ê²½ìš°
                    print(f"{model:<20} {acc:.4f}      {'---':<15} {'ğŸ† ìµœê³ '}")
                else:
                    print(f"{model:<20} {acc:.4f}      {'---':<15} {'ğŸ“Š ê¸°ì¤€'}")
            else:
                improvement = acc - baseline_accuracy
                status = ""
                if i == 0:
                    status = "ğŸ† ìµœê³ "
                elif improvement > 0:
                    status = "âœ… ê°œì„ "
                else:
                    status = "âŒ ì €ì¡°"
                    
                print(f"{model:<20} {acc:.4f}      {improvement:+.4f}        {status}")
        
        print("-"*60)
        
        this.best_model = best_model[0]
        this.best_accuracy = best_model[1]
        
        return best_model[1], best_model[0]
        
    def submit(self): #ëª¨ë¸ ë°°í¬
        """
        ìµœì ì˜ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì— ëŒ€í•œ ì˜ˆì¸¡ ê²°ê³¼ë¥¼ ìƒì„±
        
        Returns:
            DataFrame: ì˜ˆì¸¡ ê²°ê³¼ê°€ í¬í•¨ëœ ì œì¶œìš© ë°ì´í„°í”„ë ˆì„
        """
        
        print("\n" + "="*50)
        print("ğŸ“Š íƒ€ì´íƒ€ë‹‰ ìƒì¡´ì ì˜ˆì¸¡ ê²°ê³¼ ì œì¶œ ì¤€ë¹„ ğŸ“Š".center(50))
        print("="*50)
        this = self.service.dataset
        
        if not hasattr(this, 'best_model'):
            print("âŒ ëª¨ë¸ í‰ê°€ê°€ ì™„ë£Œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë¨¼ì € evaluation() ë©”ì†Œë“œë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            return None
        
        print(f"\nğŸ† {this.best_model} ëª¨ë¸ë¡œ ì œì¶œ íŒŒì¼ ìƒì„± (ì •í™•ë„: {this.best_accuracy:.4f})")
        
        # ìµœì ì˜ ëª¨ë¸(GradientBoosting) ìƒì„± ë° í•™ìŠµ
        if this.best_model == 'GradientBoosting':
            model = GradientBoostingClassifier(
                n_estimators=200,
                learning_rate=0.05,
                max_depth=4,
                min_samples_split=5,
                min_samples_leaf=2,
                subsample=0.8,
                max_features='sqrt',
                random_state=42
            )
        else:
            # ë‹¤ë¥¸ ìµœì  ëª¨ë¸ì´ ì„ íƒëœ ê²½ìš° í•´ë‹¹ ëª¨ë¸ ì‚¬ìš©
            print(f"âš ï¸ {this.best_model} ëª¨ë¸ì€ ì•„ì§ ì œì¶œ íŒŒì¼ ìƒì„± ê¸°ëŠ¥ì´ êµ¬í˜„ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("âš ï¸ GradientBoosting ëª¨ë¸ë¡œ ëŒ€ì²´í•˜ì—¬ ì œì¶œ íŒŒì¼ì„ ìƒì„±í•©ë‹ˆë‹¤.")
            model = GradientBoostingClassifier(
                n_estimators=200,
                learning_rate=0.05,
                max_depth=4,
                min_samples_split=5,
                min_samples_leaf=2,
                subsample=0.8,
                max_features='sqrt',
                random_state=42
            )
        
        # ëª¨ë¸ í•™ìŠµ
        print("ğŸ”„ ëª¨ë¸ í•™ìŠµ ì¤‘...")
        model.fit(this.train, this.label)
        
        # í…ŒìŠ¤íŠ¸ ë°ì´í„°ë¡œ ì˜ˆì¸¡
        print("ğŸ”„ í…ŒìŠ¤íŠ¸ ë°ì´í„° ì˜ˆì¸¡ ì¤‘...")
        predictions = model.predict(this.test)
        
        # ì œì¶œ íŒŒì¼ ìƒì„±
        print("ğŸ”„ ì œì¶œ íŒŒì¼ ìƒì„± ì¤‘...")
        submission = pd.DataFrame({
            'PassengerId': this.id,
            'Survived': predictions
        })
        
        # Docker í™˜ê²½ì—ì„œ ì‘ë™í•˜ëŠ” ì ˆëŒ€ ê²½ë¡œ ì‚¬ìš©
        # ì• í”Œë¦¬ì¼€ì´ì…˜ ë£¨íŠ¸ ë””ë ‰í† ë¦¬ ê¸°ì¤€ ìƒëŒ€ ê²½ë¡œ
        base_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
        output_dir = os.path.join(base_dir, 'updated_data')
        
        # updated_data í´ë”ê°€ ì—†ìœ¼ë©´ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)
        
        # íŒŒì¼ ì €ì¥
        submission_path = os.path.join(output_dir, 'submission.csv')
        submission.to_csv(submission_path, index=False)
        
        print(f"âœ… ì œì¶œ íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {submission_path}")
        print(f"ğŸ“Š ì´ {len(predictions)}ê°œì˜ ìŠ¹ê° ë°ì´í„° ì˜ˆì¸¡ ì™„ë£Œ")
        print(f"   - ìƒì¡´ ì˜ˆì¸¡ ìŠ¹ê° ìˆ˜: {sum(predictions)} ëª…")
        print(f"   - ì‚¬ë§ ì˜ˆì¸¡ ìŠ¹ê° ìˆ˜: {len(predictions) - sum(predictions)} ëª…")
        
        return submission
